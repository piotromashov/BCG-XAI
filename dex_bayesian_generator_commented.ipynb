{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from warnings import catch_warnings,simplefilter\n",
    "import pandas as pd\n",
    "import acquisition_functions as acq_functions\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import pickle\n",
    "import dex_python_utilities as du\n",
    "\n",
    "\n",
    "#distance metrics between two laternatives and a given target direction (positive or negative)\n",
    "def distance(alternative,template,positive_target):\n",
    "    if positive_target:\n",
    "        dist = alternative-template\n",
    "        dist[dist<0]=0\n",
    "    else:\n",
    "        dist =template-alternative\n",
    "        dist[dist<0]=0\n",
    "    return sum(dist)\n",
    "\n",
    "#same as distance() applied over an array of alternatives\n",
    "def distance_arr(alternative,template,positive_target):\n",
    "    if positive_target:\n",
    "        dist = alternative-template\n",
    "        dist[dist<0]=0\n",
    "    else:\n",
    "        dist =template-alternative\n",
    "        dist[dist<0]=0\n",
    "    return np.sum(dist,axis=1)\n",
    "\n",
    "#counts number of attribites that have lower (or higher for negative target) values than template\n",
    "def difference_arr(alternative,template,positive_target):\n",
    "    if positive_target:\n",
    "        diff = alternative<template\n",
    "    else:\n",
    "        diff =template>alternative\n",
    "    return np.sum(diff,axis=1)\n",
    "    \n",
    "#returns positions of attributes with differnet values than the template_numeric\n",
    "def find_changes(alternative,template_numeric,positive_target):\n",
    "    if positive_target:\n",
    "        return np.argwhere(alternative>template_numeric)\n",
    "    return np.argwhere(alternative<template_numeric)\n",
    "# distance(best_x,template_numeric,positive_target)\n",
    "\n",
    "#generates random alternatives for a given template\n",
    "def generate_min_max_alternatives_from_template_2(n,known_alternatives,template,num_changes,\n",
    "                                                  improvement=True,min_values=[],max_values=[]): \n",
    "    print(\"Generating alternatives from template.\")\n",
    "    if num_changes>len(template)/2:\n",
    "        num_changes = num_changes//2\n",
    "        \n",
    "    min_change=1\n",
    "    if num_changes==0:\n",
    "        num_changes=1\n",
    "    print(\"num_changes:\",num_changes,'min_change:',min_change)\n",
    "    \n",
    "    if len(min_values)<1:\n",
    "        min_values=2\n",
    "        max_values=0\n",
    "        print('here')\n",
    "    \n",
    "    n = 10000\n",
    "\n",
    "    samples = np.random.rand(n,len(template)) #random matrix with n samples with values between 0 and 1\n",
    "\n",
    "    max_change = max(max_values) #maximum allowed change for one attribute (e.g., from 0 to 3)\n",
    "    \n",
    "    ranges = (np.arange(0,100,100//(2*max_change+1))/100)[::-1][1:-1]\n",
    "    new_values = []\n",
    "    for n in range(max_change):\n",
    "        new_values.append(n+1)\n",
    "        new_values.append(-(n+1))\n",
    "    samples[samples>ranges[0]]=new_values[0]  \n",
    "    for k in range(len(ranges)-1):\n",
    "        samples[(samples<=ranges[k]) & (samples>ranges[k+1])]=new_values[k+1]\n",
    "\n",
    "    samples[(samples<=ranges[len(ranges)-1]) & (samples>=0)]=0\n",
    "\n",
    "    samples = samples+template #increase each row with the template (i.e., increase template values by zero,one or two)\n",
    "    \n",
    "    #remove values bigger than max value\n",
    "    while len(samples[samples>max_values])>0:\n",
    "        samples[samples>max_values] = samples[samples>max_values]-1\n",
    "    samples[samples<min_values]=0 \n",
    "\n",
    "    samples = samples.astype(int)\n",
    "    print(samples.shape)\n",
    "    \n",
    "    #increase atrute values that have lower values thetemplate\n",
    "    \n",
    "    #remove samples that are same as the template\n",
    "    samples=samples[np.sum(samples!=template,axis=1)>0] \n",
    "    print(samples.shape)\n",
    "\n",
    "    #remove samples that have more changes than max_num_changes ot less cnhanges than min_change\n",
    "    d = distance_arr(samples,template,improvement)\n",
    "    samples=samples[(d<=num_changes) & (d>=min_change)]\n",
    "    \n",
    "    print(samples.shape)\n",
    "    #remove samples that already exist in known_alternatives\n",
    "    last_idx = len(known_alternatives)\n",
    "    samples = np.vstack((known_alternatives,samples))\n",
    "    _,idx_arr = np.unique(samples,axis=0,return_index=True)\n",
    "    idx_arr = idx_arr[idx_arr>last_idx]\n",
    "    samples = samples[idx_arr]\n",
    "    print(samples.shape)\n",
    "\n",
    "    print(\"Done. Sample size:\",samples.shape)\n",
    "    return samples\n",
    "\n",
    "def update_random_alternatives(random_sample_size,known_alternatives,template_numeric,num_changes,positive_target,min_values,max_values):\n",
    "    return generate_min_max_alternatives_from_template_2(random_sample_size,known_alternatives,template_numeric,num_changes,positive_target,\n",
    "                                                        min_values,max_values)\n",
    "\n",
    "#calculate objective fuction for a list aletrnatives\n",
    "def calculate_objective_all(model,attribute_values,alternatives,template_numeric,target,output_values,positive_target,run=0):\n",
    "    cmd_eval_base = 'java -jar ./lib/DEXxSearch-1.2-dev.jar SIMPLE_EVALUATION models/'\n",
    "    alternative_path =str(run)+'_template_alternatives.json'\n",
    "    with open(alternative_path, 'w') as outfile:\n",
    "#         json.dump(str(alternatives).replace(\" \",\"\").replace(\"'\",'\"'), outfile)\n",
    "            json.dump(alternatives, outfile)\n",
    "\n",
    "#     cmd = cmd_eval_base+model+\" \"+str(alternatives).replace(\" \",\"\").replace(\"'\",'\"')\n",
    "    cmd = cmd_eval_base+model+\" \"+alternative_path\n",
    "\n",
    "    s = subprocess.check_output(cmd,shell=True)\n",
    "    str_value = json.loads(str(s)[2:-3])\n",
    "#     print(str_value)\n",
    "    evaluation_values = []\n",
    "    for val in str_value:\n",
    "        evaluation_values.append(int(output_values.index(val[0]['_'])))\n",
    "#     print(evaluation_values)\n",
    "    evaluation_values = np.array(evaluation_values)\n",
    "    \n",
    "    # check if we are moving towards the target or not.\n",
    "    if positive_target:\n",
    "        #target_eval = 1+(evaluation_values-target)\n",
    "        target_eval = evaluation_values>=target \n",
    "    else:\n",
    "        #target_eval = 1+(target-evaluation_values)\n",
    "        target_eval = evaluation_values<=target\n",
    "\n",
    "    overall_num_changes = 2*len(template_numeric)\n",
    "#     target_eval[target_eval>=2] = 1/overall_num_changes\n",
    "#     target_eval[target_eval<0] = 0\n",
    "\n",
    "#     print(target,'evaluation values:',evaluation_values)\n",
    "    alternatives_numeric = du.alternatives_to_num(alternatives,attribute_values)\n",
    "    alternatives_numeric = np.array(alternatives_numeric)\n",
    "    template_numeric = np.array(template_numeric)\n",
    "    \n",
    "    # here should go the cost of attribute changes and their weights\n",
    "    num_changes = distance_arr(alternatives_numeric,template_numeric,positive_target)\n",
    "\n",
    "    # closeness to feature space of the potential counterfactual to the initial instance.\n",
    "    relative_similarity = 1-num_changes/overall_num_changes\n",
    "    objective_value = relative_similarity*target_eval\n",
    "\n",
    "#     print('relative_similarity:',relative_similarity,'target_eval:',\n",
    "#           target_eval,'objective_value',objective_value)\n",
    "    \n",
    "    return objective_value\n",
    "\n",
    "\n",
    "       \n",
    "# surrogate or approximation for the objective function\n",
    "def surrogate(model, X):\n",
    "    # catch any warning generated when making a prediction\n",
    "    with catch_warnings():\n",
    "    # ignore generated warnings\n",
    "        simplefilter(\"ignore\")\n",
    "        return model.predict(X, return_std=True)\n",
    "\n",
    "#returns mean values and standard deviation calculated over the predictions\n",
    "#from each separate model from a given ensemble models\n",
    "def get_ensemble_scores(ens_model,X):\n",
    "    ens_predictions = []\n",
    "    for est in range(len(ens_model.estimators_)): \n",
    "        ens_predictions.append(ens_model.estimators_[est].predict(X))\n",
    "    ens_predictions = np.array(ens_predictions)\n",
    "    mu = ens_predictions.mean(axis=0)\n",
    "    return ens_predictions.mean(axis=0),ens_predictions.std(axis=0)\n",
    "\n",
    "#returns an array of predictions from each separate model in a given ensemble model\n",
    "def get_ensemble_predictions(ens_model,X):\n",
    "    ens_predictions = []\n",
    "    for est in range(len(ens_model.estimators_)): \n",
    "        ens_predictions.append(ens_model.estimators_[est].predict(X))\n",
    "    return np.array(ens_predictions)\n",
    "\n",
    "\n",
    "# returns scores caclulated with an acquisition function (see acqusition_functions.py)\n",
    "def acquisition(X, X_candidates, model):\n",
    "    yhat, _ = get_ensemble_scores(model,X)\n",
    "    best = max(yhat)\n",
    "    mu, std = get_ensemble_scores(model,X_candidates)\n",
    "    score = acq_functions.EI(mu, std, best, epsilon=.001)\n",
    "    return score\n",
    "\n",
    "# optimize the acquisition function\n",
    "from scipy.stats import norm\n",
    "def opt_acquisition(X, y, model,X_candidates,attribute_values,template_string, top_n = 10):\n",
    "    # calculate the acquisition function for each candidate\n",
    "    scores = acquisition(X, X_candidates, model)\n",
    "    # locate the index of the largest scores\n",
    "    if top_n>len(scores)//2:\n",
    "        top_n = len(scores)//2\n",
    "    ix = np.argpartition(scores, -top_n)[-top_n:] #get top_n candidates\n",
    "    best_alternative_numeric= X_candidates[ix]\n",
    "    rand_idx =np.random.randint(0,len(X_candidates),top_n) #get_random_cadidates\n",
    "    rand_alternative_numeric =  X_candidates[rand_idx]\n",
    "    \n",
    "    #remove candidates from the random candidates, as they will be available in X\n",
    "    rm_idx = np.concatenate([rand_idx,ix])\n",
    "    X_candidates = np.delete(X_candidates,rm_idx,axis=0)\n",
    "\n",
    "    #join top-n candidates and n random cadindates and calculate their string representations\n",
    "    alternatives_numeric = np.vstack([best_alternative_numeric,rand_alternative_numeric])\n",
    "    alternatives_string = []\n",
    "    k = sorted(template_string.keys())\n",
    "    for a in alternatives_numeric:\n",
    "        alternatives_string.append(du.atribute_values_to_string(a,k,attribute_values))\n",
    "        \n",
    "    return alternatives_string,alternatives_numeric,X_candidates\n",
    "\n",
    "\n",
    "#updates columns at index indx with the given values val from a given matrix A, updates eac\n",
    "#can be optimized. e.g., to avoid geenrating duplicates\n",
    "def update_per_row(A, indx, val,num_elem=1):\n",
    "    all_indx = indx[:,None] + np.arange(num_elem)\n",
    "    A[np.arange(all_indx.shape[0])[:,None], all_indx] =val\n",
    "    return A\n",
    "\n",
    "#recoursive function\n",
    "#probably can be optimized\n",
    "#generates neighbours (i.e., similar solutions with same or samlled distance values than a given alternative x\n",
    "#with respect to a templace\n",
    "#max_degree tells hown many times the function will be called\n",
    "#max_degree=1 (only x's neighbours)\n",
    "#max_degree=2 (x's neighbours and neighbours of x's neighbours)\n",
    "#max_degree=3 (x's neighbours, neighbours of x's neighbours, and neighbours of the neighbours of x's neighbours)\n",
    "def generate_neighbours(x,template,positive_target,current_degree=1,x_close=[],max_degree=2):\n",
    "    idx_arr = find_changes(x,template,positive_target)\n",
    "    while current_degree<max_degree and len(idx_arr)!=0 and len(x_close)<5000:\n",
    "        current_degree = current_degree+1\n",
    "        for position in idx_arr:\n",
    "            tmp_x = x.copy()\n",
    "            if positive_target: #this update will dicrease the distance function\n",
    "                tmp_x[position] = tmp_x[position]-1\n",
    "            else:\n",
    "                tmp_x[position] = tmp_x[position]+1\n",
    "            x_close.append(tmp_x)\n",
    "            \n",
    "            if positive_target: #this update will not change the distance function\n",
    "                idx_arr_neg  = np.argwhere(tmp_x<template)\n",
    "                values = tmp_x[idx_arr_neg]+1\n",
    "            else:\n",
    "                idx_arr_neg  = np.argwhere(tmp_x>template)\n",
    "                values = tmp_x[idx_arr_neg]-1\n",
    "            if len(idx_arr_neg)>0:\n",
    "                tmp_arr = np.repeat([tmp_x],len(idx_arr_neg),axis=0)\n",
    "                tmp_arr =update_per_row(tmp_arr,idx_arr_neg.flatten(),values)\n",
    "                x_close.extend(tmp_arr)\n",
    "            \n",
    "            generate_neighbours(tmp_x,template,positive_target,\n",
    "                                current_degree,x_close,max_degree) \n",
    "    #remove duplicates\n",
    "    if len(x_close)>0:\n",
    "        if len(x_close)>1:\n",
    "            x_close = np.unique(x_close,axis=0)\n",
    "        return list(x_close)\n",
    "    return []\n",
    "        \n",
    "#removes from \"samples\" alternatives that already exist in known_alternatives \n",
    "def remove_duplicates(samples,known_alternatives):\n",
    "    last_idx = len(known_alternatives)-1\n",
    "    samples = np.vstack((known_alternatives,samples))\n",
    "    _,idx_arr = np.unique(samples,axis=0,return_index=True) #remove duplicate samples\n",
    "    idx_arr = idx_arr[idx_arr>last_idx]\n",
    "    samples = samples[idx_arr]\n",
    "    return list(samples)\n",
    "\n",
    "#check promising_alternatives_pool \n",
    "def check_promising_alternatives(dex_model,model,alternatives,best_y,best_pool,threshold,template,template_string,attribute_values,target,output_values,positive_target,run):\n",
    "    print('checking promising_alternatives')\n",
    "    top_n = 1000\n",
    "    if len(alternatives)==0:\n",
    "        alternatives = np.array(best_pool)[:top_n]\n",
    "    else:\n",
    "        #predict the optimization function with the surogate model\n",
    "        if np.array(alternatives).ndim==1:\n",
    "            pred = model.predict(np.array(alternatives).reshape(1, -1))\n",
    "        else:\n",
    "            pred = model.predict(alternatives)\n",
    "        alternatives = np.array(alternatives)\n",
    "        print(alternatives.shape)\n",
    "\n",
    "        #get n-top ranked alternatives\n",
    "   \n",
    "        alternatives = alternatives[pred>=best_y*threshold]\n",
    "        pred = pred[pred>=best_y*threshold]\n",
    "        print(alternatives.shape)\n",
    "        if len(alternatives)>top_n:\n",
    "            ix = np.argpartition(pred, -top_n)[-top_n:] #get top_n candidates\n",
    "            alternatives= alternatives[ix]\n",
    "\n",
    "    print(alternatives.shape)\n",
    "    Y = np.repeat(best_y,len(best_pool))\n",
    "    #for each top-ranked alternative\n",
    "    #check real objective value of the alternative and its neighbours\n",
    "    if len(alternatives)>0:\n",
    "        k = sorted(template_string.keys())\n",
    "        alternatives_string = []\n",
    "        for a in alternatives:\n",
    "            alternatives_string.append(du.atribute_values_to_string(a,k,attribute_values))\n",
    "\n",
    "        #check real objective value of all alternatives\n",
    "        print(len(alternatives_string))\n",
    "        Y_tmp=calculate_objective_all(dex_model,attribute_values,alternatives_string,template,target,output_values,positive_target,run)\n",
    "        \n",
    "        alternatives = np.vstack((alternatives,best_pool))\n",
    "        Y = np.concatenate((Y_tmp,Y))\n",
    "    else:\n",
    "         alternatives = np.array(best_pool)\n",
    "\n",
    "    alternatives =  alternatives[[Y>=best_y]]\n",
    "    Y = Y[[Y>=best_y]]\n",
    "    print('candidates best_y',len(Y),alternatives.shape)\n",
    "\n",
    "    #generate neighbours\n",
    "    x_close = []\n",
    "    alternatives_tmp = alternatives\n",
    "    for i in range(10):\n",
    "        len_before = len(x_close) \n",
    "        for tmp_x in alternatives_tmp:\n",
    "            if positive_target: #this updaee would not change the distance value between the template and the alternative\n",
    "                idx_arr_neg  = np.argwhere(tmp_x<template)\n",
    "                values = tmp_x[idx_arr_neg]+1\n",
    "            else:\n",
    "                idx_arr_neg  = np.argwhere(tmp_x>template)\n",
    "                values = tmp_x[idx_arr_neg]-1\n",
    "            if len(idx_arr_neg)>0:\n",
    "                tmp_arr = np.repeat([tmp_x],len(idx_arr_neg),axis=0)\n",
    "                tmp_arr =update_per_row(tmp_arr,idx_arr_neg.flatten(),values)\n",
    "                x_close.extend(tmp_arr)\n",
    "        alternatives_tmp = x_close[len_before:]\n",
    "        if len(x_close)==len_before or len(x_close)>top_n: #ro avoid memmory error\n",
    "            break\n",
    "    #check the real objective value of the neighbours\n",
    "    if len(x_close)>0:\n",
    "        x_close =  remove_duplicates(x_close,alternatives)\n",
    "        if len(x_close)>0:  \n",
    "            print(len(x_close))\n",
    "            alternatives_string = []\n",
    "            k = sorted(template_string.keys())\n",
    "\n",
    "            for a in x_close:\n",
    "                alternatives_string.append(du.atribute_values_to_string(a,k,attribute_values))\n",
    "\n",
    "            Y_tmp=calculate_objective_all(dex_model,attribute_values,alternatives_string,template,target,output_values,positive_target,run)\n",
    "            \n",
    "            alternatives = np.vstack((alternatives,x_close))\n",
    "            Y = np.concatenate((Y,Y_tmp))\n",
    "            alternatives = alternatives[Y>=best_y]\n",
    "            Y = Y[Y>=best_y]\n",
    "    alternatives = np.unique(alternatives,axis=0)\n",
    "    print(\"unique Y:\",alternatives.shape)\n",
    "    print('best_y',len(Y))\n",
    "    return alternatives,Y\n",
    "\n",
    "\n",
    "#returns min and max values for each attribute\n",
    "def generate_minMax_constrais(attribute_values):\n",
    "    min_values = np.zeros((len(attribute_values)))\n",
    "    max_values = []\n",
    "    for av in attribute_values:\n",
    "        max_values.append(len(av)-1)\n",
    "    return min_values,np.array(max_values)\n",
    "\n",
    "def acquisition_random(X_candidates,attribute_values,template_string, top_n = 20):\n",
    "    \n",
    "    rand_idx =np.random.randint(0,len(X_candidates),top_n) #get_random_cadidates\n",
    "    rand_alternative_numeric =  X_candidates[rand_idx]\n",
    "    \n",
    "    #remove candidates from the random candidates, as they will be available in X\n",
    "    X_candidates = np.delete(X_candidates,rand_idx,axis=0)\n",
    "\n",
    "    #join top-n candidates and n random cadindates and calculate their string representations\n",
    "    alternatives_numeric = rand_alternative_numeric\n",
    "    alternatives_string = []\n",
    "    k = sorted(template_string.keys())\n",
    "    for a in alternatives_numeric:\n",
    "        alternatives_string.append(du.atribute_values_to_string(a,k,attribute_values))\n",
    "        \n",
    "    return alternatives_string,alternatives_numeric,X_candidates\n",
    "\n",
    "\n",
    "def run_generator(model,starting_alternative,template_string,output_values,target = 2, neighbours_max_degree=3,\n",
    "                  first_sample = 3,positive_target = True, local_path = \"F:/DS/\",save_results=True,run=0):\n",
    "    \n",
    "    print(\"model:\",model,'target:',target,'positive_target:',positive_target)\n",
    "    \n",
    "    #get information about the model, which features we have and their possible values\n",
    "    attributes,attribute_category,attribute_values  = du.get_model_description(model)\n",
    "    \n",
    "    #random instances for the bayesian model\n",
    "    random_sample_size=10000\n",
    "    #overall number of epochs to run the algorithm\n",
    "    num_epochs = 150 \n",
    "    #generate new samples when the size is lower than x% of the initial size\n",
    "    random_sample_size_threshold = .1 \n",
    "    #generate new samples when the average objective values has not incrased for x epochs\n",
    "    objective_zero_threshold = 3\n",
    "    #improvement on amout of epochs to stop without having improvements.\n",
    "    improvement_zero_threshold = 50   \n",
    "    current_num_changes=0\n",
    "    new_max_epoch_threshold = 50\n",
    "    min_epoch_threshold = 100\n",
    "                           \n",
    "    print()\n",
    "    print('-----------')\n",
    "    model  =\" \"+model\n",
    "    attributes,attribute_category,attribute_values  = du.get_model_description(model)\n",
    "    surrogate_model = RandomForestRegressor(1000,n_jobs=4)\n",
    "    \n",
    "    #Generate starting dataset and train the surrogate_model\n",
    "    # string representation of the initial instances.\n",
    "    X_string = du.generate_random_alternatives_DEX(first_sample,model) \n",
    "    # integer representation of each instance\n",
    "    X = du.alternatives_to_num(X_string,attribute_values)\n",
    "    # convert initial instance to numeric as well\n",
    "    template_numeric =du.atribute_values_to_num(template_string,attribute_values)\n",
    "    # objective function, for now we work with this single number (single objective optimization)\n",
    "    Y=calculate_objective_all(\n",
    "        model,\n",
    "        attribute_values,\n",
    "        X_string,\n",
    "        template_numeric,\n",
    "        target,\n",
    "        output_values, #possible output values that the DSM (or other plugin model) can have\n",
    "        positive_target, \n",
    "        run\n",
    "    )\n",
    "    \n",
    "    surrogate_model.fit(X,Y)\n",
    "    known_alternatives = X.copy() #known_alternatives to avoid duplicates\n",
    "\n",
    "    #for log\n",
    "    print('model:',model,'first_sample:',first_sample,'target:',target,'template:',starting_alternative,'positive_target:',positive_target)\n",
    "\n",
    "    #oversample current best if it carries some information\n",
    "    best_x = X[np.argmax(Y)]\n",
    "    best_x_close = []\n",
    "    best_x_close_string = []\n",
    "    current_best_Y = max(Y)\n",
    "    if current_best_Y>0:\n",
    "        current_best = X[np.argmax(Y)]\n",
    "        current_best = np.repeat([current_best],10,axis=0)\n",
    "        X = np.vstack((X, current_best))\n",
    "        actual = current_best_Y\n",
    "        actual = np.repeat(actual,10)\n",
    "        Y = np.concatenate((Y, actual))\n",
    "        \n",
    "        #save best_x neighbours to be checked in the next iteration\n",
    "        best_x = current_best[0]                \n",
    "        best_x_close = generate_neighbours(best_x,template_numeric,positive_target,1,[],neighbours_max_degree)\n",
    "\n",
    "    current_num_changes = distance(np.array(best_x),np.array(template_numeric),positive_target)-1\n",
    "    \n",
    "    i=0\n",
    "    objective_zero = 0 #counter - number of epochs without objective improvement\n",
    "    improvement_zero=0 #counter - number of epochs improvement being zero\n",
    "    secondary_counter=0 #counter - number of checks of the secondary pool\n",
    "    pool_num = []\n",
    "    pool_string = []\n",
    "    \n",
    "    # estimation of the objective function\n",
    "    EST_epoch_mean = []\n",
    "    # actual value of the objective function\n",
    "    Y_epoch_mean= []\n",
    "    BEST_alternatives_pool_arr = []\n",
    "    stoh_start_time = time.time()\n",
    "    random_alternatives = np.array([[1],[1]])\n",
    "    random_alternatives_inital_size = 1\n",
    "    \n",
    "    # giving search space for the solutions we are finding\n",
    "    neighborhood_jitter= .75\n",
    "    changes_jitter=8\n",
    "    print('neighborhood_jitter',neighborhood_jitter)\n",
    "    print('neighbours_max_degree',neighbours_max_degree)\n",
    "    \n",
    "    best_alternatives_pool = []\n",
    "    best_alternatives_pool.append(list(best_x))\n",
    "    \n",
    "    promising_alternatives_pool = []\n",
    "    promising_alternatives_pool.extend(best_x_close)\n",
    "    new_max_epoch = 0\n",
    "    x_string_arr = []\n",
    "    \n",
    "    \n",
    "    min_values, max_values = generate_minMax_constrais(attribute_values)\n",
    "    \n",
    "    while i < num_epochs:\n",
    "        print('----')\n",
    "        print(i,'epoch')\n",
    "        #helper variables\n",
    "        objective_zero = objective_zero+1\n",
    "        improvement_zero = improvement_zero+1\n",
    "        i=i+1\n",
    "        new_max_epoch = new_max_epoch+1\n",
    "        \n",
    "        #check if we have close neighbours to be checked\n",
    "        if len(best_x_close)>0:\n",
    "            print('neighbours to be checked:',len(best_x_close))\n",
    "            #obtain top 10 of the neighbours (based on the acquisition function)\n",
    "            x_string_arr,x_num_arr,_ = opt_acquisition(\n",
    "                X, #possible counterfactuals with known objective value\n",
    "                Y, #objective value for X\n",
    "                surrogate_model, #the bayesian model that is used\n",
    "                np.array(best_x_close), #neighbouring instances close to the current best\n",
    "                attribute_values, #possible feature values for each feature field (DEX related)\n",
    "                template_string  #string representation of initial instance\n",
    "            )\n",
    "            print('predicting...')\n",
    "            #logging purposes on the prediction\n",
    "            est_arr = surrogate_model.predict(x_num_arr)\n",
    "            best_x_close = []\n",
    "            best_x_close_string = []\n",
    "        else:\n",
    "            #go with random counterfactuals\n",
    "            if len(random_alternatives)<10:\n",
    "                num_changes=changes_jitter+current_num_changes\n",
    "                random_alternatives = generate_min_max_alternatives_from_template_2(random_sample_size,known_alternatives,template_numeric,num_changes,positive_target,\n",
    "                                                                                   min_values,max_values) \n",
    "                #number of unique generated instances\n",
    "                random_alternatives_inital_size = random_alternatives.shape[0]\n",
    "            if len(random_alternatives)>0:\n",
    "                x_string_arr,x_num_arr,random_alternatives = opt_acquisition(X, Y, surrogate_model,random_alternatives,attribute_values,template_string)\n",
    "                print('predicting...')\n",
    "                #logging purposes on the prediction\n",
    "                est_arr = surrogate_model.predict(x_num_arr)\n",
    "    \n",
    "        if len(x_string_arr)!=0:\n",
    "            print('calculating objective...')\n",
    "            actual_arr = []\n",
    "            actual_arr = calculate_objective_all(model,attribute_values,x_string_arr,template_numeric,target,output_values,positive_target,run)\n",
    "\n",
    "            #for log - save average estimated and real objective values \n",
    "            if len(est_arr)>10:\n",
    "                EST_epoch_mean.extend([np.mean(est_arr[:10]),np.mean(est_arr[10:])]) #first 5 are infromative samples second 5 are random samples\n",
    "                Y_epoch_mean.extend([np.mean(actual_arr[:10]),np.mean(actual_arr[10:])])\n",
    "            else:\n",
    "                EST_epoch_mean.extend([np.mean(est_arr),-1]) #first 5 are infromative samples second 5 are random samples\n",
    "                Y_epoch_mean.extend([np.mean(actual_arr),-1])\n",
    "\n",
    "            #add to known_alternatives\n",
    "            known_alternatives = np.vstack([known_alternatives,x_num_arr])\n",
    "\n",
    "            print('estimated:',np.round(est_arr,3))\n",
    "            print('actual:',np.round(actual_arr,3),np.round(max(Y),3))\n",
    "\n",
    "            # add the data to the dataset for the surogate model\n",
    "            for k in range(len(actual_arr)):\n",
    "                actual = actual_arr[k]\n",
    "                est = est_arr[k]\n",
    "                x_sample = x_num_arr[k]\n",
    "                x_string = x_string_arr[k]\n",
    "\n",
    "                if actual>0.01:\n",
    "                    objective_zero=0 #restart counter\n",
    "                   # improvement_weight = len(X)/10 #oversample improvement instance using improvement_weight\n",
    "                improvement_weight = 1\n",
    "\n",
    "                # check if close to current best, then it's candidate to keep exploring\n",
    "                if actual>=(current_best_Y*neighborhood_jitter) and actual>0.01:\n",
    "                    best_x_close_tmp = generate_neighbours(x_sample,template_numeric,positive_target,1,[],neighbours_max_degree)\n",
    "                    best_x_close.extend(best_x_close_tmp)\n",
    "\n",
    "                if actual>=current_best_Y and actual>0.01: #new estimated optimum found\n",
    "                    best_x = x_sample\n",
    "                    if actual>current_best_Y: #restart best alternatives\n",
    "                        best_alternatives_pool=[]                            \n",
    "                        BEST_alternatives_pool_arr = list(np.repeat(0,len(BEST_alternatives_pool_arr)))\n",
    "                    if list(best_x) not in best_alternatives_pool:\n",
    "                        best_alternatives_pool.append(list(best_x))\n",
    "                    new_max_epoch = 0\n",
    "\n",
    "                    improvement_zero=1 #restart counter\n",
    "                    \n",
    "                    # maybe this does not oversample? check.\n",
    "                    x_sample = np.repeat([x_sample],improvement_weight,axis=0) #oversample\n",
    "                    actual = np.repeat(actual,improvement_weight) #oversample\n",
    "                    Y = np.concatenate((Y, actual))\n",
    "                    current_best_Y = actual[0]\n",
    "                    #check if we need to decrease the mutation_rate\n",
    "                else:\n",
    "                    Y = np.concatenate((Y, [actual]))\n",
    "\n",
    "                X = np.vstack((X, x_sample))\n",
    "                X_string.append(x_string)\n",
    "\n",
    "            # retrain our bayesian model for improvement on new information.\n",
    "            surrogate_model.fit(X,Y)\n",
    "\n",
    "            \n",
    "            if len(best_x_close)>0:\n",
    "                best_x_close= remove_duplicates(best_x_close,known_alternatives)\n",
    "                if i>3: #store promising_alternatives_pool after nth learning epoch\n",
    "                    promising_alternatives_pool.extend(best_x_close)\n",
    "            # alternatives to be check after the last iteration, because the Bayesian model may be wiser then.\n",
    "            BEST_alternatives_pool_arr.append(len(best_alternatives_pool))\n",
    "            print(known_alternatives.shape,X.shape,random_alternatives.shape,new_max_epoch,len(best_x_close),len(best_alternatives_pool),len(promising_alternatives_pool))\n",
    "\n",
    "        # LAST ITERATION\n",
    "        # update the model\n",
    "        if len(x_string_arr)==0 or ((new_max_epoch>=new_max_epoch_threshold or i==num_epochs) and i>min_epoch_threshold):\n",
    "    \n",
    "            print(\"Best alternative:\",i,current_best_Y)\n",
    "            # adaptation for getting DEX on numeric and textual representation\n",
    "            m = X[np.argmax(Y)]\n",
    "            k = sorted(template_string.keys())\n",
    "            ms = du.atribute_values_to_string(m,k,attribute_values)\n",
    "\n",
    "            threshold = .9\n",
    "            promising_alternatives_pool.extend(best_alternatives_pool)\n",
    "            # final check on the last iteration, with the wiser surrogate model.\n",
    "            final_alternatives,final_y = check_promising_alternatives(model,surrogate_model,\n",
    "                    promising_alternatives_pool,current_best_Y,best_alternatives_pool,\n",
    "                    threshold,template_numeric,template_string,\n",
    "                    attribute_values,target,output_values,\n",
    "                    positive_target,run)\n",
    "\n",
    "            BEST_alternatives_pool_arr.append(len(final_alternatives))\n",
    "            stoh_duration =  time.time() - stoh_start_time\n",
    "            \n",
    "\n",
    "            print(\"stochastic\",stoh_duration,calculate_objective_all(model,attribute_values,[ms],template_numeric,target,output_values,positive_target))\n",
    "            print(\"template\",calculate_objective_all(model,attribute_values,[template_string],template_numeric,target,output_values,positive_target))\n",
    "            path = local_path+\"ML_Models/\"+model+\"_\"+str(target)+\"_\"+str(int(positive_target))+\"_\"+starting_alternative+\"_RF.pickle\"\n",
    "            if (save_results):\n",
    "                print('Saving ML model to ',path)\n",
    "                pickle.dump(surrogate_model, open(path, 'wb'))\n",
    "                print('Done')\n",
    "\n",
    "                path = local_path+\"Alternatives/\"+model+\"_\"+str(target)+\"_\"+str(int(positive_target))+\"_\"+starting_alternative+\"_alternatives.pickle\"\n",
    "                print('Saving alternatives to ',path)\n",
    "                pickle.dump([known_alternatives, promising_alternatives_pool,final_alternatives], open(path, 'wb'))\n",
    "                print('Done')\n",
    "  \n",
    "            break\n",
    "        #check if we need to create new alternatives\n",
    "        sample_too_small_bool = random_alternatives.shape[0]<random_alternatives_inital_size*random_sample_size_threshold\n",
    "        sample_too_small_bool = sample_too_small_bool or random_alternatives.shape[0]<20\n",
    "        objective_zero_bool = objective_zero_threshold<=objective_zero\n",
    "        objective_improvement_bool = improvement_zero_threshold<=improvement_zero\n",
    "          #max_mutation_rate_NOT_reached_bool = mutation_rate<max_mutation_rate\n",
    "            \n",
    "        #distance in the feature space from initial instance and the optimum solution\n",
    "        num_changes = distance(np.array(best_x),np.array(template_numeric),positive_target)-1\n",
    "        changes_improvement_bool = current_num_changes>num_changes\n",
    "        \n",
    "        #check if we need to update the pool of random alternatives because we run out of neighbours and\n",
    "        # the current pool is small OR\n",
    "        # there have'nt been any improvement in the past K epochs OR\n",
    "        # the objective function is stuck at 0 (no improvements on optimum)\n",
    "        if len(best_x_close)==0 and (changes_improvement_bool or objective_zero_bool or objective_improvement_bool):\n",
    "            current_num_changes = num_changes\n",
    "            num_changes=changes_jitter+current_num_changes\n",
    "            random_alternatives = update_random_alternatives(random_sample_size,known_alternatives,template_numeric,num_changes,positive_target,\n",
    "                                                            min_values,max_values)\n",
    "            random_alternatives_inital_size = random_alternatives.shape[0]\n",
    "            objective_zero=0\n",
    "            improvement_zero = 0    \n",
    "            \n",
    "            \n",
    "    return template_numeric,final_alternatives,BEST_alternatives_pool_arr,stoh_duration,Y_epoch_mean,EST_epoch_mean\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
